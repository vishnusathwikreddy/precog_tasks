{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjeBQQpoJJtQ",
        "outputId": "dffd5797-4ff8-4f75-e821-10e39b92f92c"
      },
      "id": "wjeBQQpoJJtQ",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install gensim -q\n",
        "!pip3 install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBcWEHQtJlII",
        "outputId": "f00b4309-2348-4f3a-c360-053d83a0542f",
        "collapsed": true
      },
      "id": "xBcWEHQtJlII",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.3.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "7c84ea39-a5cd-4084-889b-7c9d5534b5b7",
      "metadata": {
        "id": "7c84ea39-a5cd-4084-889b-7c9d5534b5b7",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "ds = load_dataset(\"PiC/phrase_similarity\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "560759be-d658-4053-a9b6-b800692953c5",
      "metadata": {
        "id": "560759be-d658-4053-a9b6-b800692953c5",
        "outputId": "26a5b67e-3f60-4a4b-e587-0eeb485fe2a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['phrase1', 'phrase2', 'sentence1', 'sentence2', 'label', 'idx'],\n",
              "        num_rows: 7004\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['phrase1', 'phrase2', 'sentence1', 'sentence2', 'label', 'idx'],\n",
              "        num_rows: 1000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['phrase1', 'phrase2', 'sentence1', 'sentence2', 'label', 'idx'],\n",
              "        num_rows: 2000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "6b55bbed-106f-49e7-bb82-b4ae79928486",
      "metadata": {
        "id": "6b55bbed-106f-49e7-bb82-b4ae79928486",
        "outputId": "87115a13-f988-4561-a2fa-9a849ccdefd6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7004 7004 7004\n"
          ]
        }
      ],
      "source": [
        "train_data = []\n",
        "train_labels = ds['train']['label']\n",
        "\n",
        "val_data = []\n",
        "val_labels = ds['validation']['label']\n",
        "\n",
        "test_data = []\n",
        "test_labels = ds['test']['label']\n",
        "\n",
        "phrase_1 = ds['train']['phrase1']\n",
        "phrase_2 = ds['train']['phrase2']\n",
        "\n",
        "val_phrase1 = ds['validation']['phrase1']\n",
        "val_phrase2 = ds['validation']['phrase2']\n",
        "\n",
        "test_phrase1 = ds['test']['phrase1']\n",
        "test_phrase2 = ds['test']['phrase2']\n",
        "\n",
        "for i in range(len(phrase_1)):\n",
        "    train_data.append([phrase_1[i], phrase_2[i]])\n",
        "\n",
        "for i in range(len(val_phrase1)):\n",
        "    val_data.append([val_phrase1[i], val_phrase2[i]])\n",
        "\n",
        "for i in range(len(test_phrase1)):\n",
        "    test_data.append([test_phrase1[i], test_phrase2[i]])\n",
        "\n",
        "print(len(phrase_1), len(phrase_2), len(train_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "dff3de6b-257a-4afc-9233-476f1a93b796",
      "metadata": {
        "id": "dff3de6b-257a-4afc-9233-476f1a93b796",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8091ef9-4717-44ec-f42a-8613bf18129d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def filtering_stopwords(data):\n",
        "    for i in range(len(data)):\n",
        "        words_1 = word_tokenize(data[i][0])\n",
        "        words_2 = word_tokenize(data[i][1])\n",
        "        filtered_1 = [w for w in words_1 if not w.lower() in stop_words]\n",
        "        filtered_2 = [w for w in words_2 if not w.lower() in stop_words]\n",
        "        data[i]=[filtered_1,filtered_2]\n",
        "    return data\n",
        "\n",
        "train_data=filtering_stopwords(train_data)\n",
        "val_data=filtering_stopwords(val_data)\n",
        "test_data=filtering_stopwords(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "ab837f66-ea5a-4abb-ace0-1c538cb64f76",
      "metadata": {
        "id": "ab837f66-ea5a-4abb-ace0-1c538cb64f76",
        "outputId": "6d6e0bd5-ad81-47ab-9829-5402043f7ed5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['newly', 'formed', 'camp'], ['recently', 'made', 'encampment']]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "train_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "f6b7f2f0-c7c3-4b6f-9d60-798b13f07176",
      "metadata": {
        "id": "f6b7f2f0-c7c3-4b6f-9d60-798b13f07176",
        "outputId": "75292660-34f0-4b19-f452-f9d4e26f8151",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['first', 'colony'], ['original', 'settlement']]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "val_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "f4d235c2-3ab2-4d41-acaf-168da3f4f75a",
      "metadata": {
        "id": "f4d235c2-3ab2-4d41-acaf-168da3f4f75a",
        "outputId": "fdf770b7-f4a1-4c35-e339-54f2ae76ecc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['air', 'position'], ['posture', 'jumping']]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "test_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "5e5d7960-6714-46f3-ab0d-75f8e34d534d",
      "metadata": {
        "id": "5e5d7960-6714-46f3-ab0d-75f8e34d534d"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "fasttext_model_path = \"/content/drive/MyDrive/FASTTEXT_embedding_model/wiki-news-300d-1M.vec\"\n",
        "embedding_model = gensim.models.KeyedVectors.load_word2vec_format(fasttext_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "33a09d69-3eaa-4781-8669-f4bff790d465",
      "metadata": {
        "id": "33a09d69-3eaa-4781-8669-f4bff790d465",
        "outputId": "1b130a59-fefd-4ca2-e2fc-003f8e630679",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7004 7004\n",
            "1000 1000\n",
            "2000 2000\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "def generate_embeddings(data, model):\n",
        "    embeddings = []\n",
        "\n",
        "    for i in range(len(data)):\n",
        "        phrase_1 = data[i][0]\n",
        "        phrase_2 = data[i][1]\n",
        "\n",
        "        phrase_1_embeddings = []\n",
        "        for word in phrase_1:\n",
        "            if word not in model:\n",
        "              phrase_1_embeddings.append(np.zeros(model.vector_size))\n",
        "            else:\n",
        "                phrase_1_embeddings.append(model[word])\n",
        "\n",
        "        phrase_2_embeddings = []\n",
        "        for word in phrase_2:\n",
        "            if word not in model:\n",
        "                phrase_1_embeddings.append(np.zeros(model.vector_size))\n",
        "            else:\n",
        "                phrase_2_embeddings.append(model[word])\n",
        "\n",
        "        embeddings.append([phrase_1_embeddings, phrase_2_embeddings])\n",
        "\n",
        "    return embeddings\n",
        "\n",
        "train_embeddings = generate_embeddings(train_data, embedding_model)\n",
        "val_embeddings = generate_embeddings(val_data, embedding_model)\n",
        "test_embeddings = generate_embeddings(test_data, embedding_model)\n",
        "\n",
        "print(len(train_embeddings),len(train_labels))\n",
        "print(len(val_embeddings),len(val_embeddings))\n",
        "print(len(test_embeddings),len(test_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "7ce35314-f65b-4173-839b-ae0fc4c34ecb",
      "metadata": {
        "id": "7ce35314-f65b-4173-839b-ae0fc4c34ecb",
        "outputId": "c1e0f93c-1934-4afc-9ca9-de85ccd7629e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "type(train_embeddings[0][0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "5e68a465-f303-4c56-9d53-42b1f1af8692",
      "metadata": {
        "id": "5e68a465-f303-4c56-9d53-42b1f1af8692"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_average_embeddings(embeddings, labels):\n",
        "    average_embeddings = []\n",
        "    filtered_labels = []\n",
        "\n",
        "    for i in range(len(embeddings)):\n",
        "        phrase1 = embeddings[i][0]\n",
        "        phrase2 = embeddings[i][1]\n",
        "\n",
        "        phrase1_average = np.zeros(300,)\n",
        "        phrase2_average = np.zeros(300,)\n",
        "\n",
        "        if len(phrase1) > 0:\n",
        "            for j in range(len(phrase1)):\n",
        "                phrase1_average += phrase1[j]\n",
        "            phrase1_average /= len(phrase1)\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        if len(phrase2) > 0:\n",
        "            for j in range(len(phrase2)):\n",
        "                phrase2_average += phrase2[j]\n",
        "            phrase2_average /= len(phrase2)\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        filtered_labels.append(labels[i])\n",
        "        average_embeddings.append([phrase1_average, phrase2_average])\n",
        "\n",
        "    return average_embeddings, filtered_labels\n",
        "\n",
        "average_train_embeddings, average_train_labels = calculate_average_embeddings(train_embeddings, train_labels)\n",
        "average_val_embeddings, average_val_labels = calculate_average_embeddings(val_embeddings, val_labels)\n",
        "average_test_embeddings, average_test_labels = calculate_average_embeddings(test_embeddings, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "a8c7b3b3-c239-43cd-a494-7b62fe56ece7",
      "metadata": {
        "id": "a8c7b3b3-c239-43cd-a494-7b62fe56ece7",
        "outputId": "47a387d8-99a9-4c43-8676-b87fc6bffbbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "len(train_embeddings[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "122e89d1-b567-4cef-854d-e2839131a35d",
      "metadata": {
        "id": "122e89d1-b567-4cef-854d-e2839131a35d",
        "outputId": "27f65fe9-a4df-4efa-f4e1-4f8a05b6b5c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy:  0.41382765531062127\n",
            "Test Accuracy:  0.4294294294294294\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def flatten_embeddings(average_embeddings):\n",
        "    flattened_embeddings = []\n",
        "    for pair in average_embeddings:\n",
        "        flattened_embeddings.append(np.concatenate(pair))\n",
        "    return flattened_embeddings\n",
        "\n",
        "train_features = flatten_embeddings(average_train_embeddings)\n",
        "val_features = flatten_embeddings(average_val_embeddings)\n",
        "test_features = flatten_embeddings(average_test_embeddings)\n",
        "\n",
        "svm_classifier = SVC(max_iter=1000)\n",
        "svm_classifier.fit(train_features, average_train_labels)\n",
        "\n",
        "val_predictions = svm_classifier.predict(val_features)\n",
        "test_predictions = svm_classifier.predict(test_features)\n",
        "\n",
        "val_accuracy = accuracy_score(average_val_labels, val_predictions)\n",
        "test_accuracy = accuracy_score(average_test_labels, test_predictions)\n",
        "print(\"Validation Accuracy: \", val_accuracy)\n",
        "print(\"Test Accuracy: \", test_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "239c0539-aa27-4f1c-bf7d-b08ae079ff0b",
      "metadata": {
        "id": "239c0539-aa27-4f1c-bf7d-b08ae079ff0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d66fbc8b-eab6-4b31-ed74-f31232dadc15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy with Random Forest:  0.14729458917835672\n",
            "Test Accuracy with Random Forest:  0.16016016016016016\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(train_features, average_train_labels)\n",
        "\n",
        "val_predictions = rf_model.predict(val_features)\n",
        "val_accuracy = accuracy_score(average_val_labels, val_predictions)\n",
        "print('Validation Accuracy with Random Forest: ', val_accuracy)\n",
        "\n",
        "test_predictions = rf_model.predict(test_features)\n",
        "test_accuracy = accuracy_score(average_test_labels, test_predictions)\n",
        "print('Test Accuracy with Random Forest: ', test_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "lr_model.fit(train_features, average_train_labels)\n",
        "\n",
        "val_predictions = lr_model.predict(val_features)\n",
        "val_accuracy = accuracy_score(average_val_labels, val_predictions)\n",
        "print('Validation Accuracy with Logistic Regression: ', val_accuracy)\n",
        "\n",
        "test_predictions = lr_model.predict(test_features)\n",
        "test_accuracy = accuracy_score(average_test_labels, test_predictions)\n",
        "print('Test Accuracy with Logistic Regression:', test_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HuhN5a6NX9o",
        "outputId": "d2ccde20-9e7c-4ab4-9f8e-b759ead2279a"
      },
      "id": "6HuhN5a6NX9o",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy with Logistic Regression:  0.38877755511022044\n",
            "Test Accuracy with Logistic Regression: 0.35935935935935936\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_model.fit(train_features, average_train_labels)\n",
        "\n",
        "val_predictions = knn_model.predict(val_features)\n",
        "val_accuracy = accuracy_score(average_val_labels, val_predictions)\n",
        "print(\"Validation Accuracy with KNN:\", val_accuracy)\n",
        "\n",
        "test_predictions = knn_model.predict(test_features)\n",
        "test_accuracy = accuracy_score(average_test_labels, test_predictions)\n",
        "print(\"Test Accuracy with KNN:\" ,test_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxpNQy0OOc5H",
        "outputId": "2c743be0-5c09-46e4-9cca-4f227b6f02e5"
      },
      "id": "FxpNQy0OOc5H",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy with KNN: 0.2755511022044088\n",
            "Test Accuracy with KNN: 0.2757757757757758\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#weighted average section\n",
        "\n",
        "#storing counts of all the words in weights dictionary\n",
        "\n",
        "weights = {}\n",
        "count = 0\n",
        "for i in range(len(train_data)):\n",
        "    phrase_1 = train_data[i][0]\n",
        "    phrase_2 = train_data[i][1]\n",
        "    for word in phrase_1:\n",
        "        if word not in weights:\n",
        "            weights[word] = 1\n",
        "        else:\n",
        "            weights[word] += 1\n",
        "        count+=1\n",
        "    for word in phrase_2:\n",
        "        if word not in weights:\n",
        "            weights[word] = 1\n",
        "        else:\n",
        "            weights[word] += 1\n",
        "        count+=1\n",
        "\n",
        "for i in range(len(val_data)):\n",
        "    phrase_1 = val_data[i][0]\n",
        "    phrase_2 = val_data[i][1]\n",
        "    for word in phrase_1:\n",
        "        if word not in weights:\n",
        "            weights[word] = 1\n",
        "        else:\n",
        "            weights[word] += 1\n",
        "        count+=1\n",
        "    for word in phrase_2:\n",
        "        if word not in weights:\n",
        "            weights[word] = 1\n",
        "        else:\n",
        "            weights[word] += 1\n",
        "        count+=1\n",
        "\n",
        "for i in range(len(test_data)):\n",
        "    phrase_1 = test_data[i][0]\n",
        "    phrase_2 = test_data[i][1]\n",
        "    for word in phrase_1:\n",
        "        if word not in weights:\n",
        "            weights[word] = 1\n",
        "        else:\n",
        "            weights[word] += 1\n",
        "        count+=1\n",
        "    for word in phrase_2:\n",
        "        if word not in weights:\n",
        "            weights[word] = 1\n",
        "        else:\n",
        "            weights[word] += 1\n",
        "        count+=1\n",
        "\n",
        "for i in weights:\n",
        "  weights[i] = weights[i]/count\n",
        "\n",
        "count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-joyuSZ1eytz",
        "outputId": "3f94c962-297a-4da0-f690-713a67be2d8c"
      },
      "id": "-joyuSZ1eytz",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42146"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights['newly']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcDZm9hKggWq",
        "outputId": "b4959e00-ffbf-47c8-aee3-d5309eeb07e9"
      },
      "id": "CcDZm9hKggWq",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0002372704408484791"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_data), len(train_embeddings))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGv0EBhEhfR6",
        "outputId": "6ea562f4-40ff-4aec-c307-bb92776bed28"
      },
      "id": "UGv0EBhEhfR6",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7004 7004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def weighted_average_embeddings(embeddings, labels, weights, data):\n",
        "    weighted_average_embeddings = []\n",
        "    filtered_labels = []\n",
        "    for i in range(len(embeddings)):\n",
        "        phrase1 = embeddings[i][0]\n",
        "        phrase2 = embeddings[i][1]\n",
        "\n",
        "        phrase_1_words = data[i][0]\n",
        "        phrase_2_words = data[i][1]\n",
        "\n",
        "        phrase1_weighted_average = np.zeros(300,)\n",
        "        phrase2_weighted_average = np.zeros(300,)\n",
        "\n",
        "        if len(phrase1) > 0:\n",
        "            for j in range(len(phrase1)):\n",
        "                phrase1_weighted_average += (weights[phrase_1_words[j]] * phrase1[j]) #multiplied with weights.\n",
        "            phrase1_weighted_average /= len(phrase1)\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        if len(phrase2) > 0:\n",
        "            for j in range(len(phrase2)):\n",
        "                phrase2_weighted_average += (weights[phrase_2_words[j]] * phrase2[j]) #multiplied with weights\n",
        "            phrase2_weighted_average /= len(phrase2)\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        filtered_labels.append(labels[i])\n",
        "        weighted_average_embeddings.append([phrase1_weighted_average, phrase2_weighted_average])\n",
        "\n",
        "    return weighted_average_embeddings, filtered_labels\n",
        "\n",
        "\n",
        "weighted_average_train_embeddings, weighted_average_train_labels = calculate_average_embeddings(train_embeddings, train_labels)\n",
        "weighted_average_val_embeddings, weighted_average_val_labels = calculate_average_embeddings(val_embeddings, val_labels)\n",
        "weighted_average_test_embeddings, weighted_average_test_labels = calculate_average_embeddings(test_embeddings, test_labels)"
      ],
      "metadata": {
        "id": "fMZ9OE-wgo4A"
      },
      "id": "fMZ9OE-wgo4A",
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_embeddings[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAo0pP3Vhn-Y",
        "outputId": "d97ea06c-a14d-48ef-f2ac-ad7d4fd52b6e"
      },
      "id": "zAo0pP3Vhn-Y",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def flatten_embeddings(average_embeddings):\n",
        "    flattened_embeddings = []\n",
        "    for pair in average_embeddings:\n",
        "        flattened_embeddings.append(np.concatenate(pair))\n",
        "    return flattened_embeddings\n",
        "\n",
        "weighted_train_features = flatten_embeddings(weighted_average_train_embeddings)\n",
        "weighted_val_features = flatten_embeddings(weighted_average_val_embeddings)\n",
        "weighted_test_features = flatten_embeddings(weighted_average_test_embeddings)\n",
        "\n",
        "svm_classifier = SVC(max_iter=1000)\n",
        "svm_classifier.fit(weighted_train_features, weighted_average_train_labels)\n",
        "\n",
        "weighted_val_predictions = svm_classifier.predict(weighted_val_features)\n",
        "weighted_test_predictions = svm_classifier.predict(weighted_test_features)\n",
        "\n",
        "weighted_val_accuracy = accuracy_score(weighted_average_val_labels, weighted_val_predictions)\n",
        "weighted_test_accuracy = accuracy_score(weighted_average_test_labels, weighted_test_predictions)\n",
        "print(\"Validation Accuracy: \", weighted_val_accuracy)\n",
        "print(\"Test Accuracy: \", weighted_test_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKqGl_ghkd7H",
        "outputId": "6c1b545b-1c5d-4918-b946-3b3a7112ca7b"
      },
      "id": "cKqGl_ghkd7H",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy:  0.41382765531062127\n",
            "Test Accuracy:  0.4294294294294294\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(weighted_train_features, weighted_average_train_labels)\n",
        "\n",
        "weighted_val_predictions = rf_model.predict(weighted_val_features)\n",
        "val_accuracy = accuracy_score(weighted_average_val_labels, weighted_val_predictions)\n",
        "print('Validation Accuracy with Random Forest: ', val_accuracy)\n",
        "\n",
        "weighted_test_predictions = rf_model.predict(weighted_test_features)\n",
        "test_accuracy = accuracy_score(weighted_average_test_labels, weighted_test_predictions)\n",
        "print('Test Accuracy with Random Forest: ', test_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nE8LvUatk5Ro",
        "outputId": "b50f3808-045d-439f-c813-3d15db8e5191"
      },
      "id": "nE8LvUatk5Ro",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy with Random Forest:  0.14729458917835672\n",
            "Test Accuracy with Random Forest:  0.16016016016016016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "lr_model.fit(weighted_train_features, weighted_average_train_labels)\n",
        "\n",
        "weighted_val_predictions = lr_model.predict(weighted_val_features)\n",
        "val_accuracy = accuracy_score(weighted_average_val_labels, weighted_val_predictions)\n",
        "print('Validation Accuracy with Logistic Regression: ', val_accuracy)\n",
        "\n",
        "weighted_test_predictions = lr_model.predict(weighted_test_features)\n",
        "test_accuracy = accuracy_score(weighted_average_test_labels, weighted_test_predictions)\n",
        "print('Test Accuracy with Logistic Regression:', test_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBfed1p_l96v",
        "outputId": "343423fc-97fe-4614-b5ac-813449b776a2"
      },
      "id": "pBfed1p_l96v",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy with Logistic Regression:  0.38877755511022044\n",
            "Test Accuracy with Logistic Regression: 0.35935935935935936\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_model.fit(weighted_train_features, weighted_average_train_labels)\n",
        "\n",
        "weighted_val_predictions = knn_model.predict(weighted_val_features)\n",
        "val_accuracy = accuracy_score(weighted_average_val_labels, weighted_val_predictions)\n",
        "print(\"Validation Accuracy with KNN:\", val_accuracy)\n",
        "\n",
        "weighted_test_predictions = knn_model.predict(weighted_test_features)\n",
        "test_accuracy = accuracy_score(weighted_average_test_labels, weighted_test_predictions)\n",
        "print(\"Test Accuracy with KNN:\" ,test_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPc0jnfwm750",
        "outputId": "0f3d5e90-4a96-4530-c687-9702aab1785e"
      },
      "id": "zPc0jnfwm750",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy with KNN: 0.2755511022044088\n",
            "Test Accuracy with KNN: 0.2757757757757758\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}